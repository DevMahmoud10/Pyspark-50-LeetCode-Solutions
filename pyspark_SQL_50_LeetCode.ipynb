{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "RcP3ADM_BXgE",
        "p19kAvcBBdOk",
        "OLIcbWApFonb",
        "jeT-UB5cDgWj",
        "aJjTDU7WFiu5",
        "ypUuhWKyFc-l",
        "QO7rtWm7ADi_",
        "xGZM-FoMBNqa",
        "IFBjXuAEUpOh",
        "MQt0N_rwenxb"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# SQL 50 LeetCode in Pyspark"
      ],
      "metadata": {
        "id": "hWD8GI4yC67S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UuynyjyZ2Vit",
        "outputId": "56cce32f-d31b-4f8b-919d-92eb582d816e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark==3.5.1\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark==3.5.1) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=8068c758765925b79b5121a7ce574bef4b8a7efea38c47437aa04f3a2c2c3e5c\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark==3.5.1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import Window\n",
        "import pyspark.sql.functions as F\n",
        "spark=SparkSession.builder.appName(\"SQL 50 LeetCode\").getOrCreate()"
      ],
      "metadata": {
        "id": "koecuf-52blV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# #1 [1757. Recyclable and Low Fat Products](https://leetcode.com/problems/recyclable-and-low-fat-products/?envType=study-plan-v2&envId=top-sql-50)"
      ],
      "metadata": {
        "id": "RcP3ADM_BXgE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pyspark Data Schema"
      ],
      "metadata": {
        "id": "p19kAvcBBdOk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.master(\"local[1]\").appName(\"example\").getOrCreate()\n",
        "\n",
        "# Provided data\n",
        "data = [\n",
        "    ['0', 'Y', 'N'],\n",
        "    ['1', 'Y', 'Y'],\n",
        "    ['2', 'N', 'Y'],\n",
        "    ['3', 'Y', 'Y'],\n",
        "    ['4', 'N', 'N']\n",
        "]\n",
        "\n",
        "# Define the schema for the DataFrame\n",
        "schema = StructType([\n",
        "    StructField(\"product_id\", StringType(), True),\n",
        "    StructField(\"low_fats\", StringType(), True),\n",
        "    StructField(\"recyclable\", StringType(), True)\n",
        "])\n",
        "\n",
        "# Convert the data to a PySpark DataFrame\n",
        "products_df = spark.createDataFrame(data, schema=schema)\n",
        "\n",
        "# Show the DataFrame\n",
        "products_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "erlGVTWkBoez",
        "outputId": "92a3cf2f-9a2e-4bb5-9717-cf8f5b32d2a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------+----------+\n",
            "|product_id|low_fats|recyclable|\n",
            "+----------+--------+----------+\n",
            "|         0|       Y|         N|\n",
            "|         1|       Y|         Y|\n",
            "|         2|       N|         Y|\n",
            "|         3|       Y|         Y|\n",
            "|         4|       N|         N|\n",
            "+----------+--------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution"
      ],
      "metadata": {
        "id": "Nwxi22PIBkMX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "products_df.filter((F.col(\"low_fats\")=='Y') & (F.col(\"recyclable\")=='Y')).select(\"product_id\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJG3-yulB4yb",
        "outputId": "5b6d979d-8188-41a0-d521-bb53a354f47b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+\n",
            "|product_id|\n",
            "+----------+\n",
            "|         1|\n",
            "|         3|\n",
            "+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# #2 [584. Find Customer Referee](https://leetcode.com/problems/find-customer-referee/?envType=study-plan-v2&envId=top-sql-50)"
      ],
      "metadata": {
        "id": "OLIcbWApFonb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pyspark Data Schema"
      ],
      "metadata": {
        "id": "X8GEx2OrFqXO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.master(\"local[1]\").appName(\"example\").getOrCreate()\n",
        "\n",
        "# Provided data\n",
        "data = [\n",
        "    [1, 'Will', None],\n",
        "    [2, 'Jane', None],\n",
        "    [3, 'Alex', 2],\n",
        "    [4, 'Bill', None],\n",
        "    [5, 'Zack', 1],\n",
        "    [6, 'Mark', 2]\n",
        "]\n",
        "\n",
        "# Define the schema for the DataFrame\n",
        "schema = StructType([\n",
        "    StructField(\"id\", IntegerType(), True),\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"referee_id\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "# Convert the data to a PySpark DataFrame\n",
        "customer_df = spark.createDataFrame(data, schema=schema)\n",
        "\n",
        "# Show the DataFrame\n",
        "customer_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cN2vulhiFtk4",
        "outputId": "2685a2ad-e6d7-4e17-b9c5-c53e0d1b5a49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+----------+\n",
            "| id|name|referee_id|\n",
            "+---+----+----------+\n",
            "|  1|Will|      NULL|\n",
            "|  2|Jane|      NULL|\n",
            "|  3|Alex|         2|\n",
            "|  4|Bill|      NULL|\n",
            "|  5|Zack|         1|\n",
            "|  6|Mark|         2|\n",
            "+---+----+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution"
      ],
      "metadata": {
        "id": "Of2_-TyBFriR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "customer_df.filter((F.col('referee_id')!=2)|(F.col('referee_id').isNull())).select(\"name\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bX07E6SWFtHO",
        "outputId": "8dd05c85-187f-417b-fcd7-f8b6823ad918"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+\n",
            "|name|\n",
            "+----+\n",
            "|Will|\n",
            "|Jane|\n",
            "|Bill|\n",
            "|Zack|\n",
            "+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# #3 [595. Big Countries](https://leetcode.com/problems/big-countries/?envType=study-plan-v2&envId=top-sql-50)"
      ],
      "metadata": {
        "id": "jeT-UB5cDgWj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pyspark Data Schema"
      ],
      "metadata": {
        "id": "aJjTDU7WFiu5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.master(\"local[1]\").appName(\"example\").getOrCreate()\n",
        "\n",
        "# Provided data\n",
        "data = [\n",
        "    ['Afghanistan', 'Asia', 652230, 25500100, 20343000000],\n",
        "    ['Albania', 'Europe', 28748, 2831741, 12960000000],\n",
        "    ['Algeria', 'Africa', 2381741, 37100000, 188681000000],\n",
        "    ['Andorra', 'Europe', 468, 78115, 3712000000],\n",
        "    ['Angola', 'Africa', 1246700, 20609294, 100990000000]\n",
        "]\n",
        "\n",
        "# Define the schema for the DataFrame\n",
        "schema = StructType([\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"continent\", StringType(), True),\n",
        "    StructField(\"area\", LongType(), True),\n",
        "    StructField(\"population\", LongType(), True),\n",
        "    StructField(\"gdp\", LongType(), True)\n",
        "])\n",
        "\n",
        "# Convert the data to a PySpark DataFrame\n",
        "world_df = spark.createDataFrame(data, schema=schema)\n",
        "\n",
        "# Show the DataFrame\n",
        "world_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t58_90ZNDfY2",
        "outputId": "46004b54-29fd-447f-f9d8-5a0035fbd472"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+---------+-------+----------+------------+\n",
            "|       name|continent|   area|population|         gdp|\n",
            "+-----------+---------+-------+----------+------------+\n",
            "|Afghanistan|     Asia| 652230|  25500100| 20343000000|\n",
            "|    Albania|   Europe|  28748|   2831741| 12960000000|\n",
            "|    Algeria|   Africa|2381741|  37100000|188681000000|\n",
            "|    Andorra|   Europe|    468|     78115|  3712000000|\n",
            "|     Angola|   Africa|1246700|  20609294|100990000000|\n",
            "+-----------+---------+-------+----------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution"
      ],
      "metadata": {
        "id": "ypUuhWKyFc-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "world_df.filter((F.col('area')>=3000000)|(F.col('population')>=25000000)).select(\"name\",\"population\",\"area\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIqjfYjSDyGv",
        "outputId": "76965cb7-4c68-4c5b-e6e5-b632b6976c51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------+-------+\n",
            "|       name|population|   area|\n",
            "+-----------+----------+-------+\n",
            "|Afghanistan|  25500100| 652230|\n",
            "|    Algeria|  37100000|2381741|\n",
            "+-----------+----------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# #4  [1148. Article Views I](https://leetcode.com/problems/article-views-i/?envType=study-plan-v2&envId=top-sql-50)"
      ],
      "metadata": {
        "id": "QO7rtWm7ADi_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pyspark Data Schema"
      ],
      "metadata": {
        "id": "xGZM-FoMBNqa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, DateType\n",
        "from pyspark.sql.functions import to_date\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.master(\"local[1]\").appName(\"example\").getOrCreate()\n",
        "\n",
        "# Provided data\n",
        "data = [\n",
        "    [1, 3, 5, '2019-08-01'],\n",
        "    [1, 3, 6, '2019-08-02'],\n",
        "    [2, 7, 7, '2019-08-01'],\n",
        "    [2, 7, 6, '2019-08-02'],\n",
        "    [4, 7, 1, '2019-07-22'],\n",
        "    [3, 4, 4, '2019-07-21'],\n",
        "    [3, 4, 4, '2019-07-21']\n",
        "]\n",
        "\n",
        "# Define the schema for the DataFrame\n",
        "schema = StructType([\n",
        "    StructField(\"article_id\", IntegerType(), True),\n",
        "    StructField(\"author_id\", IntegerType(), True),\n",
        "    StructField(\"viewer_id\", IntegerType(), True),\n",
        "    StructField(\"view_date\", StringType(), True)\n",
        "])\n",
        "\n",
        "# Convert the data to a PySpark DataFrame\n",
        "views_df = spark.createDataFrame(data, schema=schema)\n",
        "\n",
        "# Convert the view_date column from string to date\n",
        "views_df = views_df.withColumn(\"view_date\", to_date(\"view_date\"))\n",
        "\n",
        "# Show the DataFrame\n",
        "views_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDAyk1t_6Pe_",
        "outputId": "54fdb223-8ac9-4f52-863f-846ae212baa7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---------+---------+----------+\n",
            "|article_id|author_id|viewer_id| view_date|\n",
            "+----------+---------+---------+----------+\n",
            "|         1|        3|        5|2019-08-01|\n",
            "|         1|        3|        6|2019-08-02|\n",
            "|         2|        7|        7|2019-08-01|\n",
            "|         2|        7|        6|2019-08-02|\n",
            "|         4|        7|        1|2019-07-22|\n",
            "|         3|        4|        4|2019-07-21|\n",
            "|         3|        4|        4|2019-07-21|\n",
            "+----------+---------+---------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution"
      ],
      "metadata": {
        "id": "EYhWhas9AaMc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "views_df.where(F.col(\"author_id\")==F.col(\"viewer_id\")).select(\"author_id\").drop_duplicates().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Ivg-mma6jLz",
        "outputId": "7643f025-3ecf-421a-8cae-c6668d35fe59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|author_id|\n",
            "+---------+\n",
            "|        7|\n",
            "|        4|\n",
            "+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fy8n7-u6-aln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# #5 [1683. Invalid Tweets](https://leetcode.com/problems/invalid-tweets/?envType=study-plan-v2&envId=top-sql-50)"
      ],
      "metadata": {
        "id": "IFBjXuAEUpOh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.master(\"local[1]\").appName(\"example\").getOrCreate()\n",
        "\n",
        "# Provided data\n",
        "data = [\n",
        "    [1, 'Vote for Biden'],\n",
        "    [2, 'Let us make America great again!']\n",
        "]\n",
        "\n",
        "# Define the schema for the DataFrame\n",
        "schema = StructType([\n",
        "    StructField(\"tweet_id\", IntegerType(), True),\n",
        "    StructField(\"content\", StringType(), True)\n",
        "])\n",
        "\n",
        "# Convert the data to a PySpark DataFrame\n",
        "tweets_df = spark.createDataFrame(data, schema=schema)\n",
        "\n",
        "# Show the DataFrame\n",
        "tweets_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Wg5qhX-ViPf",
        "outputId": "6c83d544-7128-46e2-aa22-76ab001701b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+--------------------+\n",
            "|tweet_id|             content|\n",
            "+--------+--------------------+\n",
            "|       1|      Vote for Biden|\n",
            "|       2|Let us make Ameri...|\n",
            "+--------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution"
      ],
      "metadata": {
        "id": "9ZrAtV88Uqnc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_df.filter(F.length('content')>15).select('tweet_id').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpgJE-AAWTYd",
        "outputId": "d3eabb78-e033-4258-bd48-6e0424a9cfd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+\n",
            "|tweet_id|\n",
            "+--------+\n",
            "|       2|\n",
            "+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# #6 [1378. Replace Employee ID With The Unique Identifier](https://leetcode.com/problems/replace-employee-id-with-the-unique-identifier/description/?envType=study-plan-v2&envId=top-sql-50)"
      ],
      "metadata": {
        "id": "MQt0N_rwenxb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.master(\"local[1]\").appName(\"example\").getOrCreate()\n",
        "\n",
        "# Provided data for employees DataFrame\n",
        "employees_data = [\n",
        "    [1, 'Alice'],\n",
        "    [7, 'Bob'],\n",
        "    [11, 'Meir'],\n",
        "    [90, 'Winston'],\n",
        "    [3, 'Jonathan']\n",
        "]\n",
        "\n",
        "# Provided data for employee_uni DataFrame\n",
        "employee_uni_data = [\n",
        "    [3, 1],\n",
        "    [11, 2],\n",
        "    [90, 3]\n",
        "]\n",
        "\n",
        "# Create employees DataFrame\n",
        "employees_df = spark.createDataFrame(employees_data, schema=['id', 'name'])\n",
        "\n",
        "# Create employee_uni DataFrame\n",
        "employee_uni_df = spark.createDataFrame(employee_uni_data, schema=['id', 'unique_id'])\n",
        "\n",
        "# Show the DataFrames\n",
        "employees_df.show()\n",
        "employee_uni_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9fNMDgHeq4o",
        "outputId": "aec7d0a0-d9df-450a-8363-53ddaa0369e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------+\n",
            "| id|    name|\n",
            "+---+--------+\n",
            "|  1|   Alice|\n",
            "|  7|     Bob|\n",
            "| 11|    Meir|\n",
            "| 90| Winston|\n",
            "|  3|Jonathan|\n",
            "+---+--------+\n",
            "\n",
            "+---+---------+\n",
            "| id|unique_id|\n",
            "+---+---------+\n",
            "|  3|        1|\n",
            "| 11|        2|\n",
            "| 90|        3|\n",
            "+---+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution"
      ],
      "metadata": {
        "id": "F_t0_aKZetQ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "employees_df.join(employee_uni_df, how='left', on='id').select('unique_id','name').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3iPBn7DersA",
        "outputId": "1b13ae53-1e31-4400-f9e7-88cf36069d99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------+\n",
            "|unique_id|    name|\n",
            "+---------+--------+\n",
            "|     NULL|     Bob|\n",
            "|     NULL|   Alice|\n",
            "|        1|Jonathan|\n",
            "|        2|    Meir|\n",
            "|        3| Winston|\n",
            "+---------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QRdhsSgef_yJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}