{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "p19kAvcBBdOk",
        "Nwxi22PIBkMX",
        "X8GEx2OrFqXO",
        "Of2_-TyBFriR",
        "ypUuhWKyFc-l",
        "xGZM-FoMBNqa",
        "xKStUrB5wNru",
        "GdAwPnPGwSoC",
        "F_t0_aKZetQ-",
        "Fjk_U9V2shK6",
        "8tPxwRlt0Ebz",
        "3RKeccw70Cuw",
        "PLfdVwr2eaCw"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# SQL 50 LeetCode in Pyspark"
      ],
      "metadata": {
        "id": "hWD8GI4yC67S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UuynyjyZ2Vit",
        "outputId": "1314b691-43da-4a0e-b9e7-955216c495f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark==3.5.1 in /usr/local/lib/python3.10/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark==3.5.1) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark==3.5.1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import Window\n",
        "import pyspark.sql.functions as F\n",
        "spark=SparkSession.builder.appName(\"SQL 50 LeetCode\").getOrCreate()"
      ],
      "metadata": {
        "id": "koecuf-52blV"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [#1 Recyclable and Low Fat Products](https://leetcode.com/problems/recyclable-and-low-fat-products/?envType=study-plan-v2&envId=top-sql-50)"
      ],
      "metadata": {
        "id": "RcP3ADM_BXgE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pyspark Data Schema"
      ],
      "metadata": {
        "id": "p19kAvcBBdOk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.master(\"local[1]\").appName(\"example\").getOrCreate()\n",
        "\n",
        "# Provided data\n",
        "data = [\n",
        "    ['0', 'Y', 'N'],\n",
        "    ['1', 'Y', 'Y'],\n",
        "    ['2', 'N', 'Y'],\n",
        "    ['3', 'Y', 'Y'],\n",
        "    ['4', 'N', 'N']\n",
        "]\n",
        "\n",
        "# Define the schema for the DataFrame\n",
        "schema = StructType([\n",
        "    StructField(\"product_id\", StringType(), True),\n",
        "    StructField(\"low_fats\", StringType(), True),\n",
        "    StructField(\"recyclable\", StringType(), True)\n",
        "])\n",
        "\n",
        "# Convert the data to a PySpark DataFrame\n",
        "products_df = spark.createDataFrame(data, schema=schema)\n",
        "\n",
        "# Show the DataFrame\n",
        "products_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "erlGVTWkBoez",
        "outputId": "98c4367b-6c28-4a09-d932-515364c82c4b"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------+----------+\n",
            "|product_id|low_fats|recyclable|\n",
            "+----------+--------+----------+\n",
            "|         0|       Y|         N|\n",
            "|         1|       Y|         Y|\n",
            "|         2|       N|         Y|\n",
            "|         3|       Y|         Y|\n",
            "|         4|       N|         N|\n",
            "+----------+--------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution"
      ],
      "metadata": {
        "id": "Nwxi22PIBkMX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "products_df.filter((F.col(\"low_fats\")=='Y') & (F.col(\"recyclable\")=='Y')).select(\"product_id\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJG3-yulB4yb",
        "outputId": "b8ae69f7-a421-4360-e6d7-1e34bf0e3772"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+\n",
            "|product_id|\n",
            "+----------+\n",
            "|         1|\n",
            "|         3|\n",
            "+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [#2 Find Customer Referee](https://leetcode.com/problems/find-customer-referee/?envType=study-plan-v2&envId=top-sql-50)"
      ],
      "metadata": {
        "id": "OLIcbWApFonb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pyspark Data Schema"
      ],
      "metadata": {
        "id": "X8GEx2OrFqXO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.master(\"local[1]\").appName(\"example\").getOrCreate()\n",
        "\n",
        "# Provided data\n",
        "data = [\n",
        "    [1, 'Will', None],\n",
        "    [2, 'Jane', None],\n",
        "    [3, 'Alex', 2],\n",
        "    [4, 'Bill', None],\n",
        "    [5, 'Zack', 1],\n",
        "    [6, 'Mark', 2]\n",
        "]\n",
        "\n",
        "# Define the schema for the DataFrame\n",
        "schema = StructType([\n",
        "    StructField(\"id\", IntegerType(), True),\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"referee_id\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "# Convert the data to a PySpark DataFrame\n",
        "customer_df = spark.createDataFrame(data, schema=schema)\n",
        "\n",
        "# Show the DataFrame\n",
        "customer_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cN2vulhiFtk4",
        "outputId": "255340a9-6c1e-4e0e-ff46-e288fb7a0804"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+----------+\n",
            "| id|name|referee_id|\n",
            "+---+----+----------+\n",
            "|  1|Will|      NULL|\n",
            "|  2|Jane|      NULL|\n",
            "|  3|Alex|         2|\n",
            "|  4|Bill|      NULL|\n",
            "|  5|Zack|         1|\n",
            "|  6|Mark|         2|\n",
            "+---+----+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution"
      ],
      "metadata": {
        "id": "Of2_-TyBFriR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "customer_df.filter((F.col('referee_id')!=2)|(F.col('referee_id').isNull())).select(\"name\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bX07E6SWFtHO",
        "outputId": "9196a0a5-00af-480c-cde9-ab2ed8c36ec9"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+\n",
            "|name|\n",
            "+----+\n",
            "|Will|\n",
            "|Jane|\n",
            "|Bill|\n",
            "|Zack|\n",
            "+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [#3 Big Countries](https://leetcode.com/problems/big-countries/?envType=study-plan-v2&envId=top-sql-50)"
      ],
      "metadata": {
        "id": "jeT-UB5cDgWj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pyspark Data Schema"
      ],
      "metadata": {
        "id": "aJjTDU7WFiu5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.master(\"local[1]\").appName(\"example\").getOrCreate()\n",
        "\n",
        "# Provided data\n",
        "data = [\n",
        "    ['Afghanistan', 'Asia', 652230, 25500100, 20343000000],\n",
        "    ['Albania', 'Europe', 28748, 2831741, 12960000000],\n",
        "    ['Algeria', 'Africa', 2381741, 37100000, 188681000000],\n",
        "    ['Andorra', 'Europe', 468, 78115, 3712000000],\n",
        "    ['Angola', 'Africa', 1246700, 20609294, 100990000000]\n",
        "]\n",
        "\n",
        "# Define the schema for the DataFrame\n",
        "schema = StructType([\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"continent\", StringType(), True),\n",
        "    StructField(\"area\", LongType(), True),\n",
        "    StructField(\"population\", LongType(), True),\n",
        "    StructField(\"gdp\", LongType(), True)\n",
        "])\n",
        "\n",
        "# Convert the data to a PySpark DataFrame\n",
        "world_df = spark.createDataFrame(data, schema=schema)\n",
        "\n",
        "# Show the DataFrame\n",
        "world_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t58_90ZNDfY2",
        "outputId": "994fa1f4-6999-4714-bd02-4de5402a3efa"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+---------+-------+----------+------------+\n",
            "|       name|continent|   area|population|         gdp|\n",
            "+-----------+---------+-------+----------+------------+\n",
            "|Afghanistan|     Asia| 652230|  25500100| 20343000000|\n",
            "|    Albania|   Europe|  28748|   2831741| 12960000000|\n",
            "|    Algeria|   Africa|2381741|  37100000|188681000000|\n",
            "|    Andorra|   Europe|    468|     78115|  3712000000|\n",
            "|     Angola|   Africa|1246700|  20609294|100990000000|\n",
            "+-----------+---------+-------+----------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution"
      ],
      "metadata": {
        "id": "ypUuhWKyFc-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "world_df.filter((F.col('area')>=3000000)|(F.col('population')>=25000000)).select(\"name\",\"population\",\"area\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIqjfYjSDyGv",
        "outputId": "a2330112-04f0-4a10-e903-e78bdcfe2572"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------+-------+\n",
            "|       name|population|   area|\n",
            "+-----------+----------+-------+\n",
            "|Afghanistan|  25500100| 652230|\n",
            "|    Algeria|  37100000|2381741|\n",
            "+-----------+----------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [#4  Article Views I](https://leetcode.com/problems/article-views-i/?envType=study-plan-v2&envId=top-sql-50)"
      ],
      "metadata": {
        "id": "QO7rtWm7ADi_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pyspark Data Schema"
      ],
      "metadata": {
        "id": "xGZM-FoMBNqa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, DateType\n",
        "from pyspark.sql.functions import to_date\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.master(\"local[1]\").appName(\"example\").getOrCreate()\n",
        "\n",
        "# Provided data\n",
        "data = [\n",
        "    [1, 3, 5, '2019-08-01'],\n",
        "    [1, 3, 6, '2019-08-02'],\n",
        "    [2, 7, 7, '2019-08-01'],\n",
        "    [2, 7, 6, '2019-08-02'],\n",
        "    [4, 7, 1, '2019-07-22'],\n",
        "    [3, 4, 4, '2019-07-21'],\n",
        "    [3, 4, 4, '2019-07-21']\n",
        "]\n",
        "\n",
        "# Define the schema for the DataFrame\n",
        "schema = StructType([\n",
        "    StructField(\"article_id\", IntegerType(), True),\n",
        "    StructField(\"author_id\", IntegerType(), True),\n",
        "    StructField(\"viewer_id\", IntegerType(), True),\n",
        "    StructField(\"view_date\", StringType(), True)\n",
        "])\n",
        "\n",
        "# Convert the data to a PySpark DataFrame\n",
        "views_df = spark.createDataFrame(data, schema=schema)\n",
        "\n",
        "# Convert the view_date column from string to date\n",
        "views_df = views_df.withColumn(\"view_date\", to_date(\"view_date\"))\n",
        "\n",
        "# Show the DataFrame\n",
        "views_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDAyk1t_6Pe_",
        "outputId": "a5f4d4c7-4995-4c05-83e1-0da390edf0d2"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---------+---------+----------+\n",
            "|article_id|author_id|viewer_id| view_date|\n",
            "+----------+---------+---------+----------+\n",
            "|         1|        3|        5|2019-08-01|\n",
            "|         1|        3|        6|2019-08-02|\n",
            "|         2|        7|        7|2019-08-01|\n",
            "|         2|        7|        6|2019-08-02|\n",
            "|         4|        7|        1|2019-07-22|\n",
            "|         3|        4|        4|2019-07-21|\n",
            "|         3|        4|        4|2019-07-21|\n",
            "+----------+---------+---------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution"
      ],
      "metadata": {
        "id": "EYhWhas9AaMc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "views_df.where(F.col(\"author_id\")==F.col(\"viewer_id\")).select(\"author_id\").drop_duplicates().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Ivg-mma6jLz",
        "outputId": "60787a9e-3dd5-4e0a-d586-3e8935ec08f3"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|author_id|\n",
            "+---------+\n",
            "|        7|\n",
            "|        4|\n",
            "+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fy8n7-u6-aln"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [#5 Invalid Tweets](https://leetcode.com/problems/invalid-tweets/?envType=study-plan-v2&envId=top-sql-50)"
      ],
      "metadata": {
        "id": "IFBjXuAEUpOh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pyspark Data Schema"
      ],
      "metadata": {
        "id": "xKStUrB5wNru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.master(\"local[1]\").appName(\"example\").getOrCreate()\n",
        "\n",
        "# Provided data\n",
        "data = [\n",
        "    [1, 'Vote for Biden'],\n",
        "    [2, 'Let us make America great again!']\n",
        "]\n",
        "\n",
        "# Define the schema for the DataFrame\n",
        "schema = StructType([\n",
        "    StructField(\"tweet_id\", IntegerType(), True),\n",
        "    StructField(\"content\", StringType(), True)\n",
        "])\n",
        "\n",
        "# Convert the data to a PySpark DataFrame\n",
        "tweets_df = spark.createDataFrame(data, schema=schema)\n",
        "\n",
        "# Show the DataFrame\n",
        "tweets_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Wg5qhX-ViPf",
        "outputId": "5bf1f876-543f-4420-f2fb-928579f7b342"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+--------------------+\n",
            "|tweet_id|             content|\n",
            "+--------+--------------------+\n",
            "|       1|      Vote for Biden|\n",
            "|       2|Let us make Ameri...|\n",
            "+--------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution"
      ],
      "metadata": {
        "id": "9ZrAtV88Uqnc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_df.filter(F.length('content')>15).select('tweet_id').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpgJE-AAWTYd",
        "outputId": "f7e7374d-7fca-4920-f1b6-8748f2a37636"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+\n",
            "|tweet_id|\n",
            "+--------+\n",
            "|       2|\n",
            "+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [#6 Replace Employee ID With The Unique Identifier](https://leetcode.com/problems/replace-employee-id-with-the-unique-identifier/description/?envType=study-plan-v2&envId=top-sql-50)"
      ],
      "metadata": {
        "id": "MQt0N_rwenxb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pyspark Data Schema"
      ],
      "metadata": {
        "id": "GdAwPnPGwSoC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.master(\"local[1]\").appName(\"example\").getOrCreate()\n",
        "\n",
        "# Provided data for employees DataFrame\n",
        "employees_data = [\n",
        "    [1, 'Alice'],\n",
        "    [7, 'Bob'],\n",
        "    [11, 'Meir'],\n",
        "    [90, 'Winston'],\n",
        "    [3, 'Jonathan']\n",
        "]\n",
        "\n",
        "# Provided data for employee_uni DataFrame\n",
        "employee_uni_data = [\n",
        "    [3, 1],\n",
        "    [11, 2],\n",
        "    [90, 3]\n",
        "]\n",
        "\n",
        "# Create employees DataFrame\n",
        "employees_df = spark.createDataFrame(employees_data, schema=['id', 'name'])\n",
        "\n",
        "# Create employee_uni DataFrame\n",
        "employee_uni_df = spark.createDataFrame(employee_uni_data, schema=['id', 'unique_id'])\n",
        "\n",
        "# Show the DataFrames\n",
        "employees_df.show()\n",
        "employee_uni_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9fNMDgHeq4o",
        "outputId": "a9517733-1460-48dd-8bcc-f781a12a531c"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------+\n",
            "| id|    name|\n",
            "+---+--------+\n",
            "|  1|   Alice|\n",
            "|  7|     Bob|\n",
            "| 11|    Meir|\n",
            "| 90| Winston|\n",
            "|  3|Jonathan|\n",
            "+---+--------+\n",
            "\n",
            "+---+---------+\n",
            "| id|unique_id|\n",
            "+---+---------+\n",
            "|  3|        1|\n",
            "| 11|        2|\n",
            "| 90|        3|\n",
            "+---+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution"
      ],
      "metadata": {
        "id": "F_t0_aKZetQ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "employees_df.join(employee_uni_df, how='left', on='id').select('unique_id','name').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3iPBn7DersA",
        "outputId": "53aa0f32-a477-41a2-e9ea-6d0dfc3f5353"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------+\n",
            "|unique_id|    name|\n",
            "+---------+--------+\n",
            "|     NULL|     Bob|\n",
            "|     NULL|   Alice|\n",
            "|        1|Jonathan|\n",
            "|        2|    Meir|\n",
            "|        3| Winston|\n",
            "+---------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [#7 Product Sales Analysis I](https://leetcode.com/problems/product-sales-analysis-i/?envType=study-plan-v2&envId=top-sql-50)"
      ],
      "metadata": {
        "id": "Fjk_U9V2shK6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.master(\"local[1]\").appName(\"example\").getOrCreate()\n",
        "\n",
        "# Provided data for sales DataFrame\n",
        "sales_data = [\n",
        "    [1, 100, 2008, 10, 5000],\n",
        "    [2, 100, 2009, 12, 5000],\n",
        "    [7, 200, 2011, 15, 9000]\n",
        "]\n",
        "\n",
        "# Provided data for product DataFrame\n",
        "product_data = [\n",
        "    [100, 'Nokia'],\n",
        "    [200, 'Apple'],\n",
        "    [300, 'Samsung']\n",
        "]\n",
        "\n",
        "# Define the schema for sales DataFrame\n",
        "sales_schema = StructType([\n",
        "    StructField(\"sale_id\", IntegerType(), True),\n",
        "    StructField(\"product_id\", IntegerType(), True),\n",
        "    StructField(\"year\", IntegerType(), True),\n",
        "    StructField(\"quantity\", IntegerType(), True),\n",
        "    StructField(\"price\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "# Define the schema for product DataFrame\n",
        "product_schema = StructType([\n",
        "    StructField(\"product_id\", IntegerType(), True),\n",
        "    StructField(\"product_name\", StringType(), True)\n",
        "])\n",
        "\n",
        "# Create sales DataFrame\n",
        "sales_df = spark.createDataFrame(sales_data, schema=sales_schema)\n",
        "\n",
        "# Create product DataFrame\n",
        "product_df = spark.createDataFrame(product_data, schema=product_schema)\n",
        "\n",
        "# Show the DataFrames\n",
        "sales_df.show()\n",
        "product_df.show()\n"
      ],
      "metadata": {
        "id": "QRdhsSgef_yJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a4bb683-ac73-4298-a323-51e3810efd1d"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+----+--------+-----+\n",
            "|sale_id|product_id|year|quantity|price|\n",
            "+-------+----------+----+--------+-----+\n",
            "|      1|       100|2008|      10| 5000|\n",
            "|      2|       100|2009|      12| 5000|\n",
            "|      7|       200|2011|      15| 9000|\n",
            "+-------+----------+----+--------+-----+\n",
            "\n",
            "+----------+------------+\n",
            "|product_id|product_name|\n",
            "+----------+------------+\n",
            "|       100|       Nokia|\n",
            "|       200|       Apple|\n",
            "|       300|     Samsung|\n",
            "+----------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solutions"
      ],
      "metadata": {
        "id": "uKMPbfAXzSAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sales_df.join(product_df, how='left', on='product_id').groupby(['product_name','year']).agg(F.sum('price').alias('price')).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMCv5YrjsgRV",
        "outputId": "4bc9491f-7b5f-46ce-90eb-58565e0c20ad"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+----+-----+\n",
            "|product_name|year|price|\n",
            "+------------+----+-----+\n",
            "|       Apple|2011| 9000|\n",
            "|       Nokia|2008| 5000|\n",
            "|       Nokia|2009| 5000|\n",
            "+------------+----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SgiGADMrxWoP"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [#8 Customer Who Visited but Did Not Make Any Transactions](https://leetcode.com/problems/customer-who-visited-but-did-not-make-any-transactions/?envType=study-plan-v2&envId=top-sql-50)"
      ],
      "metadata": {
        "id": "OBPiTCHVzpry"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pyspark Data Schema"
      ],
      "metadata": {
        "id": "8tPxwRlt0Ebz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.master(\"local[1]\").appName(\"example\").getOrCreate()\n",
        "\n",
        "# Provided data for visits DataFrame\n",
        "visits_data = [\n",
        "    [1, 23],\n",
        "    [2, 9],\n",
        "    [4, 30],\n",
        "    [5, 54],\n",
        "    [6, 96],\n",
        "    [7, 54],\n",
        "    [8, 54]\n",
        "]\n",
        "\n",
        "# Provided data for transactions DataFrame\n",
        "transactions_data = [\n",
        "    [2, 5, 310],\n",
        "    [3, 5, 300],\n",
        "    [9, 5, 200],\n",
        "    [12, 1, 910],\n",
        "    [13, 2, 970]\n",
        "]\n",
        "\n",
        "# Create visits DataFrame\n",
        "visits_df = spark.createDataFrame(visits_data, schema=['visit_id', 'customer_id'])\n",
        "\n",
        "# Create transactions DataFrame\n",
        "transactions_df = spark.createDataFrame(transactions_data, schema=['transaction_id', 'visit_id', 'amount'])\n",
        "\n",
        "# Show the DataFrames\n",
        "visits_df.show()\n",
        "transactions_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPHpssVIzrX5",
        "outputId": "59e3cf72-8c34-46bf-8b78-905f64ace536"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----------+\n",
            "|visit_id|customer_id|\n",
            "+--------+-----------+\n",
            "|       1|         23|\n",
            "|       2|          9|\n",
            "|       4|         30|\n",
            "|       5|         54|\n",
            "|       6|         96|\n",
            "|       7|         54|\n",
            "|       8|         54|\n",
            "+--------+-----------+\n",
            "\n",
            "+--------------+--------+------+\n",
            "|transaction_id|visit_id|amount|\n",
            "+--------------+--------+------+\n",
            "|             2|       5|   310|\n",
            "|             3|       5|   300|\n",
            "|             9|       5|   200|\n",
            "|            12|       1|   910|\n",
            "|            13|       2|   970|\n",
            "+--------------+--------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution"
      ],
      "metadata": {
        "id": "3RKeccw70Cuw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_visits_trans_df=visits_df.join(transactions_df, how='left', on='visit_id').groupby(\n",
        "                                                                  ['customer_id','visit_id']).agg(\n",
        "                                                                  F.count('transaction_id').alias('transactions_count'))\n",
        "user_visits_trans_df.where(F.col('transactions_count')==0).groupby(['customer_id']).agg(F.count('customer_id').alias('count_no_trans')).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRuijhUQzsGH",
        "outputId": "0a14ccc4-fd14-4776-e28a-baf383db035c"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------------+\n",
            "|customer_id|count_no_trans|\n",
            "+-----------+--------------+\n",
            "|         54|             2|\n",
            "|         96|             1|\n",
            "|         30|             1|\n",
            "+-----------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [#9 Rising Temperature](https://leetcode.com/problems/rising-temperature/?envType=study-plan-v2&envId=top-sql-50)"
      ],
      "metadata": {
        "id": "PLfdVwr2eaCw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pyspark Data Schema"
      ],
      "metadata": {
        "id": "FhYrm5DmezIM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, DateType\n",
        "from datetime import datetime\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.master(\"local[1]\").appName(\"example\").getOrCreate()\n",
        "\n",
        "# Provided data for weather DataFrame\n",
        "data = [\n",
        "    [1, '2015-01-01', 10],\n",
        "    [2, '2015-01-02', 25],\n",
        "    [3, '2015-01-03', 20],\n",
        "    [4, '2015-01-04', 30]\n",
        "]\n",
        "\n",
        "# Define the schema for the DataFrame\n",
        "schema = StructType([\n",
        "    StructField(\"id\", IntegerType(), True),\n",
        "    StructField(\"recordDate\", DateType(), True),\n",
        "    StructField(\"temperature\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "# Convert the date strings to proper date objects\n",
        "formatted_data = [[row[0], datetime.strptime(row[1], '%Y-%m-%d').date(), row[2]] for row in data]\n",
        "\n",
        "# Convert the data to a PySpark DataFrame\n",
        "weather_df = spark.createDataFrame(formatted_data, schema=schema)\n",
        "\n",
        "# Show the DataFrame\n",
        "weather_df.show()\n"
      ],
      "metadata": {
        "id": "qLuhMjZA5Ulx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d03e5893-21a7-4d83-a75f-134abc7d6771"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+-----------+\n",
            "| id|recordDate|temperature|\n",
            "+---+----------+-----------+\n",
            "|  1|2015-01-01|         10|\n",
            "|  2|2015-01-02|         25|\n",
            "|  3|2015-01-03|         20|\n",
            "|  4|2015-01-04|         30|\n",
            "+---+----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution"
      ],
      "metadata": {
        "id": "WN4BpnuRhQcE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weather_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jdr-3ImQl43b",
        "outputId": "00f611dc-dfde-441a-c7bf-7eaa8710502d"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+-----------+\n",
            "| id|recordDate|temperature|\n",
            "+---+----------+-----------+\n",
            "|  1|2015-01-01|         10|\n",
            "|  2|2015-01-02|         25|\n",
            "|  3|2015-01-03|         20|\n",
            "|  4|2015-01-04|         30|\n",
            "+---+----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weather_df = weather_df.withColumn('next_day', F.date_add(weather_df['recordDate'], 1))\n",
        "joined_df = weather_df.alias(\"weather1\").join(\n",
        "                                              weather_df.alias(\"weather2\"), on=(F.col(\"weather1.recordDate\") == F.col(\"weather2.next_day\")), how='left'\n",
        "                                              ).where(F.col('weather1.temperature')>F.col('weather2.temperature')).select('weather1.id')\n",
        "joined_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzK1BebEeXRb",
        "outputId": "008fbfee-b0fb-44f7-a68c-bde53cb70eae"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+\n",
            "| id|\n",
            "+---+\n",
            "|  2|\n",
            "|  4|\n",
            "+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [#10 Average Time of Process per Machine](https://leetcode.com/problems/average-time-of-process-per-machine/?envType=study-plan-v2&envId=top-sql-50)"
      ],
      "metadata": {
        "id": "5joil0NgmsIy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Python Data Schema"
      ],
      "metadata": {
        "id": "Vcl7dvxYsyIa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.master(\"local[1]\").appName(\"example\").getOrCreate()\n",
        "\n",
        "# Provided data for activity DataFrame\n",
        "data = [\n",
        "    [0, 0, 'start', 0.712],\n",
        "    [0, 0, 'end', 1.52],\n",
        "    [0, 1, 'start', 3.14],\n",
        "    [0, 1, 'end', 4.12],\n",
        "    [1, 0, 'start', 0.55],\n",
        "    [1, 0, 'end', 1.55],\n",
        "    [1, 1, 'start', 0.43],\n",
        "    [1, 1, 'end', 1.42],\n",
        "    [2, 0, 'start', 4.1],\n",
        "    [2, 0, 'end', 4.512],\n",
        "    [2, 1, 'start', 2.5],\n",
        "    [2, 1, 'end', 5.0]\n",
        "]\n",
        "\n",
        "# Define the schema for the DataFrame\n",
        "schema = StructType([\n",
        "    StructField(\"machine_id\", IntegerType(), True),\n",
        "    StructField(\"process_id\", IntegerType(), True),\n",
        "    StructField(\"activity_type\", StringType(), True),\n",
        "    StructField(\"timestamp\", FloatType(), True)\n",
        "])\n",
        "\n",
        "# Convert the data to a PySpark DataFrame\n",
        "activity_df = spark.createDataFrame(data, schema=schema)\n",
        "\n",
        "# Show the DataFrame\n",
        "activity_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9I-03ALKjIvh",
        "outputId": "11dd627b-33de-4d7a-8ae2-eba140684e47"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+-------------+---------+\n",
            "|machine_id|process_id|activity_type|timestamp|\n",
            "+----------+----------+-------------+---------+\n",
            "|         0|         0|        start|    0.712|\n",
            "|         0|         0|          end|     1.52|\n",
            "|         0|         1|        start|     3.14|\n",
            "|         0|         1|          end|     4.12|\n",
            "|         1|         0|        start|     0.55|\n",
            "|         1|         0|          end|     1.55|\n",
            "|         1|         1|        start|     0.43|\n",
            "|         1|         1|          end|     1.42|\n",
            "|         2|         0|        start|      4.1|\n",
            "|         2|         0|          end|    4.512|\n",
            "|         2|         1|        start|      2.5|\n",
            "|         2|         1|          end|      5.0|\n",
            "+----------+----------+-------------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution"
      ],
      "metadata": {
        "id": "yC1HkeMJsvte"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_activity_df = activity_df.where(F.col('activity_type') == 'start').alias('start_activity_df')\n",
        "end_activity_df = activity_df.where(F.col('activity_type') == 'end').alias('end_activity_df')\n",
        "\n",
        "joined_df = start_activity_df.join(end_activity_df,\n",
        "                                    on=((F.col('start_activity_df.machine_id') == F.col('end_activity_df.machine_id')) &\n",
        "                                        (F.col('start_activity_df.process_id') == F.col('end_activity_df.process_id'))),\n",
        "                                    how='inner')\n",
        "joined_df = joined_df.withColumn('diff', F.col('end_activity_df.timestamp') - F.col('start_activity_df.timestamp'))\n",
        "processed_time_df = joined_df.groupby('start_activity_df.machine_id').agg(F.round(F.avg('diff'),3).alias('processing_time'))\n",
        "processed_time_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVcylgnGqu7q",
        "outputId": "8d2b3d0e-00ba-491c-e396-07fc00baec15"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---------------+\n",
            "|machine_id|processing_time|\n",
            "+----------+---------------+\n",
            "|         1|          0.995|\n",
            "|         2|          1.456|\n",
            "|         0|          0.894|\n",
            "+----------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mnQvKy7mpwGe"
      },
      "execution_count": 65,
      "outputs": []
    }
  ]
}