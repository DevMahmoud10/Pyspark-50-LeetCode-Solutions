{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "RcP3ADM_BXgE",
        "p19kAvcBBdOk",
        "Nwxi22PIBkMX",
        "OLIcbWApFonb",
        "X8GEx2OrFqXO",
        "Of2_-TyBFriR",
        "jeT-UB5cDgWj",
        "ypUuhWKyFc-l",
        "QO7rtWm7ADi_",
        "xGZM-FoMBNqa",
        "IFBjXuAEUpOh",
        "xKStUrB5wNru",
        "GdAwPnPGwSoC",
        "F_t0_aKZetQ-"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# SQL 50 LeetCode in Pyspark"
      ],
      "metadata": {
        "id": "hWD8GI4yC67S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UuynyjyZ2Vit",
        "outputId": "72e6bf72-0dde-4504-f7b9-6c524a1e91cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark==3.5.1\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark==3.5.1) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=198217d1261b735e10c62c82d37390d2ac12982fc30909978989ebb2de62f2f8\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark==3.5.1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import Window\n",
        "import pyspark.sql.functions as F\n",
        "spark=SparkSession.builder.appName(\"SQL 50 LeetCode\").getOrCreate()"
      ],
      "metadata": {
        "id": "koecuf-52blV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# #1 [Recyclable and Low Fat Products](https://leetcode.com/problems/recyclable-and-low-fat-products/?envType=study-plan-v2&envId=top-sql-50)"
      ],
      "metadata": {
        "id": "RcP3ADM_BXgE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pyspark Data Schema"
      ],
      "metadata": {
        "id": "p19kAvcBBdOk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.master(\"local[1]\").appName(\"example\").getOrCreate()\n",
        "\n",
        "# Provided data\n",
        "data = [\n",
        "    ['0', 'Y', 'N'],\n",
        "    ['1', 'Y', 'Y'],\n",
        "    ['2', 'N', 'Y'],\n",
        "    ['3', 'Y', 'Y'],\n",
        "    ['4', 'N', 'N']\n",
        "]\n",
        "\n",
        "# Define the schema for the DataFrame\n",
        "schema = StructType([\n",
        "    StructField(\"product_id\", StringType(), True),\n",
        "    StructField(\"low_fats\", StringType(), True),\n",
        "    StructField(\"recyclable\", StringType(), True)\n",
        "])\n",
        "\n",
        "# Convert the data to a PySpark DataFrame\n",
        "products_df = spark.createDataFrame(data, schema=schema)\n",
        "\n",
        "# Show the DataFrame\n",
        "products_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "erlGVTWkBoez",
        "outputId": "92a3cf2f-9a2e-4bb5-9717-cf8f5b32d2a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------+----------+\n",
            "|product_id|low_fats|recyclable|\n",
            "+----------+--------+----------+\n",
            "|         0|       Y|         N|\n",
            "|         1|       Y|         Y|\n",
            "|         2|       N|         Y|\n",
            "|         3|       Y|         Y|\n",
            "|         4|       N|         N|\n",
            "+----------+--------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution"
      ],
      "metadata": {
        "id": "Nwxi22PIBkMX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "products_df.filter((F.col(\"low_fats\")=='Y') & (F.col(\"recyclable\")=='Y')).select(\"product_id\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJG3-yulB4yb",
        "outputId": "5b6d979d-8188-41a0-d521-bb53a354f47b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+\n",
            "|product_id|\n",
            "+----------+\n",
            "|         1|\n",
            "|         3|\n",
            "+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# #2 [Find Customer Referee](https://leetcode.com/problems/find-customer-referee/?envType=study-plan-v2&envId=top-sql-50)"
      ],
      "metadata": {
        "id": "OLIcbWApFonb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pyspark Data Schema"
      ],
      "metadata": {
        "id": "X8GEx2OrFqXO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.master(\"local[1]\").appName(\"example\").getOrCreate()\n",
        "\n",
        "# Provided data\n",
        "data = [\n",
        "    [1, 'Will', None],\n",
        "    [2, 'Jane', None],\n",
        "    [3, 'Alex', 2],\n",
        "    [4, 'Bill', None],\n",
        "    [5, 'Zack', 1],\n",
        "    [6, 'Mark', 2]\n",
        "]\n",
        "\n",
        "# Define the schema for the DataFrame\n",
        "schema = StructType([\n",
        "    StructField(\"id\", IntegerType(), True),\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"referee_id\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "# Convert the data to a PySpark DataFrame\n",
        "customer_df = spark.createDataFrame(data, schema=schema)\n",
        "\n",
        "# Show the DataFrame\n",
        "customer_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cN2vulhiFtk4",
        "outputId": "2685a2ad-e6d7-4e17-b9c5-c53e0d1b5a49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+----------+\n",
            "| id|name|referee_id|\n",
            "+---+----+----------+\n",
            "|  1|Will|      NULL|\n",
            "|  2|Jane|      NULL|\n",
            "|  3|Alex|         2|\n",
            "|  4|Bill|      NULL|\n",
            "|  5|Zack|         1|\n",
            "|  6|Mark|         2|\n",
            "+---+----+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution"
      ],
      "metadata": {
        "id": "Of2_-TyBFriR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "customer_df.filter((F.col('referee_id')!=2)|(F.col('referee_id').isNull())).select(\"name\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bX07E6SWFtHO",
        "outputId": "8dd05c85-187f-417b-fcd7-f8b6823ad918"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+\n",
            "|name|\n",
            "+----+\n",
            "|Will|\n",
            "|Jane|\n",
            "|Bill|\n",
            "|Zack|\n",
            "+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# #3 [Big Countries](https://leetcode.com/problems/big-countries/?envType=study-plan-v2&envId=top-sql-50)"
      ],
      "metadata": {
        "id": "jeT-UB5cDgWj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pyspark Data Schema"
      ],
      "metadata": {
        "id": "aJjTDU7WFiu5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.master(\"local[1]\").appName(\"example\").getOrCreate()\n",
        "\n",
        "# Provided data\n",
        "data = [\n",
        "    ['Afghanistan', 'Asia', 652230, 25500100, 20343000000],\n",
        "    ['Albania', 'Europe', 28748, 2831741, 12960000000],\n",
        "    ['Algeria', 'Africa', 2381741, 37100000, 188681000000],\n",
        "    ['Andorra', 'Europe', 468, 78115, 3712000000],\n",
        "    ['Angola', 'Africa', 1246700, 20609294, 100990000000]\n",
        "]\n",
        "\n",
        "# Define the schema for the DataFrame\n",
        "schema = StructType([\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"continent\", StringType(), True),\n",
        "    StructField(\"area\", LongType(), True),\n",
        "    StructField(\"population\", LongType(), True),\n",
        "    StructField(\"gdp\", LongType(), True)\n",
        "])\n",
        "\n",
        "# Convert the data to a PySpark DataFrame\n",
        "world_df = spark.createDataFrame(data, schema=schema)\n",
        "\n",
        "# Show the DataFrame\n",
        "world_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t58_90ZNDfY2",
        "outputId": "46004b54-29fd-447f-f9d8-5a0035fbd472"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+---------+-------+----------+------------+\n",
            "|       name|continent|   area|population|         gdp|\n",
            "+-----------+---------+-------+----------+------------+\n",
            "|Afghanistan|     Asia| 652230|  25500100| 20343000000|\n",
            "|    Albania|   Europe|  28748|   2831741| 12960000000|\n",
            "|    Algeria|   Africa|2381741|  37100000|188681000000|\n",
            "|    Andorra|   Europe|    468|     78115|  3712000000|\n",
            "|     Angola|   Africa|1246700|  20609294|100990000000|\n",
            "+-----------+---------+-------+----------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution"
      ],
      "metadata": {
        "id": "ypUuhWKyFc-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "world_df.filter((F.col('area')>=3000000)|(F.col('population')>=25000000)).select(\"name\",\"population\",\"area\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIqjfYjSDyGv",
        "outputId": "76965cb7-4c68-4c5b-e6e5-b632b6976c51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------+-------+\n",
            "|       name|population|   area|\n",
            "+-----------+----------+-------+\n",
            "|Afghanistan|  25500100| 652230|\n",
            "|    Algeria|  37100000|2381741|\n",
            "+-----------+----------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# #4  [Article Views I](https://leetcode.com/problems/article-views-i/?envType=study-plan-v2&envId=top-sql-50)"
      ],
      "metadata": {
        "id": "QO7rtWm7ADi_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pyspark Data Schema"
      ],
      "metadata": {
        "id": "xGZM-FoMBNqa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, DateType\n",
        "from pyspark.sql.functions import to_date\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.master(\"local[1]\").appName(\"example\").getOrCreate()\n",
        "\n",
        "# Provided data\n",
        "data = [\n",
        "    [1, 3, 5, '2019-08-01'],\n",
        "    [1, 3, 6, '2019-08-02'],\n",
        "    [2, 7, 7, '2019-08-01'],\n",
        "    [2, 7, 6, '2019-08-02'],\n",
        "    [4, 7, 1, '2019-07-22'],\n",
        "    [3, 4, 4, '2019-07-21'],\n",
        "    [3, 4, 4, '2019-07-21']\n",
        "]\n",
        "\n",
        "# Define the schema for the DataFrame\n",
        "schema = StructType([\n",
        "    StructField(\"article_id\", IntegerType(), True),\n",
        "    StructField(\"author_id\", IntegerType(), True),\n",
        "    StructField(\"viewer_id\", IntegerType(), True),\n",
        "    StructField(\"view_date\", StringType(), True)\n",
        "])\n",
        "\n",
        "# Convert the data to a PySpark DataFrame\n",
        "views_df = spark.createDataFrame(data, schema=schema)\n",
        "\n",
        "# Convert the view_date column from string to date\n",
        "views_df = views_df.withColumn(\"view_date\", to_date(\"view_date\"))\n",
        "\n",
        "# Show the DataFrame\n",
        "views_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDAyk1t_6Pe_",
        "outputId": "54fdb223-8ac9-4f52-863f-846ae212baa7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---------+---------+----------+\n",
            "|article_id|author_id|viewer_id| view_date|\n",
            "+----------+---------+---------+----------+\n",
            "|         1|        3|        5|2019-08-01|\n",
            "|         1|        3|        6|2019-08-02|\n",
            "|         2|        7|        7|2019-08-01|\n",
            "|         2|        7|        6|2019-08-02|\n",
            "|         4|        7|        1|2019-07-22|\n",
            "|         3|        4|        4|2019-07-21|\n",
            "|         3|        4|        4|2019-07-21|\n",
            "+----------+---------+---------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution"
      ],
      "metadata": {
        "id": "EYhWhas9AaMc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "views_df.where(F.col(\"author_id\")==F.col(\"viewer_id\")).select(\"author_id\").drop_duplicates().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Ivg-mma6jLz",
        "outputId": "7643f025-3ecf-421a-8cae-c6668d35fe59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|author_id|\n",
            "+---------+\n",
            "|        7|\n",
            "|        4|\n",
            "+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fy8n7-u6-aln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# #5 [Invalid Tweets](https://leetcode.com/problems/invalid-tweets/?envType=study-plan-v2&envId=top-sql-50)"
      ],
      "metadata": {
        "id": "IFBjXuAEUpOh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pyspark Data Schema"
      ],
      "metadata": {
        "id": "xKStUrB5wNru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.master(\"local[1]\").appName(\"example\").getOrCreate()\n",
        "\n",
        "# Provided data\n",
        "data = [\n",
        "    [1, 'Vote for Biden'],\n",
        "    [2, 'Let us make America great again!']\n",
        "]\n",
        "\n",
        "# Define the schema for the DataFrame\n",
        "schema = StructType([\n",
        "    StructField(\"tweet_id\", IntegerType(), True),\n",
        "    StructField(\"content\", StringType(), True)\n",
        "])\n",
        "\n",
        "# Convert the data to a PySpark DataFrame\n",
        "tweets_df = spark.createDataFrame(data, schema=schema)\n",
        "\n",
        "# Show the DataFrame\n",
        "tweets_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Wg5qhX-ViPf",
        "outputId": "6c83d544-7128-46e2-aa22-76ab001701b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+--------------------+\n",
            "|tweet_id|             content|\n",
            "+--------+--------------------+\n",
            "|       1|      Vote for Biden|\n",
            "|       2|Let us make Ameri...|\n",
            "+--------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution"
      ],
      "metadata": {
        "id": "9ZrAtV88Uqnc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_df.filter(F.length('content')>15).select('tweet_id').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpgJE-AAWTYd",
        "outputId": "d3eabb78-e033-4258-bd48-6e0424a9cfd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+\n",
            "|tweet_id|\n",
            "+--------+\n",
            "|       2|\n",
            "+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# #6 [Replace Employee ID With The Unique Identifier](https://leetcode.com/problems/replace-employee-id-with-the-unique-identifier/description/?envType=study-plan-v2&envId=top-sql-50)"
      ],
      "metadata": {
        "id": "MQt0N_rwenxb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pyspark Data Schema"
      ],
      "metadata": {
        "id": "GdAwPnPGwSoC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.master(\"local[1]\").appName(\"example\").getOrCreate()\n",
        "\n",
        "# Provided data for employees DataFrame\n",
        "employees_data = [\n",
        "    [1, 'Alice'],\n",
        "    [7, 'Bob'],\n",
        "    [11, 'Meir'],\n",
        "    [90, 'Winston'],\n",
        "    [3, 'Jonathan']\n",
        "]\n",
        "\n",
        "# Provided data for employee_uni DataFrame\n",
        "employee_uni_data = [\n",
        "    [3, 1],\n",
        "    [11, 2],\n",
        "    [90, 3]\n",
        "]\n",
        "\n",
        "# Create employees DataFrame\n",
        "employees_df = spark.createDataFrame(employees_data, schema=['id', 'name'])\n",
        "\n",
        "# Create employee_uni DataFrame\n",
        "employee_uni_df = spark.createDataFrame(employee_uni_data, schema=['id', 'unique_id'])\n",
        "\n",
        "# Show the DataFrames\n",
        "employees_df.show()\n",
        "employee_uni_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9fNMDgHeq4o",
        "outputId": "aec7d0a0-d9df-450a-8363-53ddaa0369e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------+\n",
            "| id|    name|\n",
            "+---+--------+\n",
            "|  1|   Alice|\n",
            "|  7|     Bob|\n",
            "| 11|    Meir|\n",
            "| 90| Winston|\n",
            "|  3|Jonathan|\n",
            "+---+--------+\n",
            "\n",
            "+---+---------+\n",
            "| id|unique_id|\n",
            "+---+---------+\n",
            "|  3|        1|\n",
            "| 11|        2|\n",
            "| 90|        3|\n",
            "+---+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution"
      ],
      "metadata": {
        "id": "F_t0_aKZetQ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "employees_df.join(employee_uni_df, how='left', on='id').select('unique_id','name').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3iPBn7DersA",
        "outputId": "1b13ae53-1e31-4400-f9e7-88cf36069d99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------+\n",
            "|unique_id|    name|\n",
            "+---------+--------+\n",
            "|     NULL|     Bob|\n",
            "|     NULL|   Alice|\n",
            "|        1|Jonathan|\n",
            "|        2|    Meir|\n",
            "|        3| Winston|\n",
            "+---------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# #7 [Product Sales Analysis I](https://leetcode.com/problems/product-sales-analysis-i/?envType=study-plan-v2&envId=top-sql-50)"
      ],
      "metadata": {
        "id": "Fjk_U9V2shK6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.master(\"local[1]\").appName(\"example\").getOrCreate()\n",
        "\n",
        "# Provided data for sales DataFrame\n",
        "sales_data = [\n",
        "    [1, 100, 2008, 10, 5000],\n",
        "    [2, 100, 2009, 12, 5000],\n",
        "    [7, 200, 2011, 15, 9000]\n",
        "]\n",
        "\n",
        "# Provided data for product DataFrame\n",
        "product_data = [\n",
        "    [100, 'Nokia'],\n",
        "    [200, 'Apple'],\n",
        "    [300, 'Samsung']\n",
        "]\n",
        "\n",
        "# Define the schema for sales DataFrame\n",
        "sales_schema = StructType([\n",
        "    StructField(\"sale_id\", IntegerType(), True),\n",
        "    StructField(\"product_id\", IntegerType(), True),\n",
        "    StructField(\"year\", IntegerType(), True),\n",
        "    StructField(\"quantity\", IntegerType(), True),\n",
        "    StructField(\"price\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "# Define the schema for product DataFrame\n",
        "product_schema = StructType([\n",
        "    StructField(\"product_id\", IntegerType(), True),\n",
        "    StructField(\"product_name\", StringType(), True)\n",
        "])\n",
        "\n",
        "# Create sales DataFrame\n",
        "sales_df = spark.createDataFrame(sales_data, schema=sales_schema)\n",
        "\n",
        "# Create product DataFrame\n",
        "product_df = spark.createDataFrame(product_data, schema=product_schema)\n",
        "\n",
        "# Show the DataFrames\n",
        "sales_df.show()\n",
        "product_df.show()\n"
      ],
      "metadata": {
        "id": "QRdhsSgef_yJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0ae4e0a-4d7f-4fd7-cec7-335af7cd103c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+----+--------+-----+\n",
            "|sale_id|product_id|year|quantity|price|\n",
            "+-------+----------+----+--------+-----+\n",
            "|      1|       100|2008|      10| 5000|\n",
            "|      2|       100|2009|      12| 5000|\n",
            "|      7|       200|2011|      15| 9000|\n",
            "+-------+----------+----+--------+-----+\n",
            "\n",
            "+----------+------------+\n",
            "|product_id|product_name|\n",
            "+----------+------------+\n",
            "|       100|       Nokia|\n",
            "|       200|       Apple|\n",
            "|       300|     Samsung|\n",
            "+----------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solutions"
      ],
      "metadata": {
        "id": "uKMPbfAXzSAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sales_df.join(product_df, how='left', on='product_id').groupby(['product_name','year']).agg(F.sum('price').alias('price')).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMCv5YrjsgRV",
        "outputId": "381f8b5c-e280-4d4c-b1b7-4ff4fba14aa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+----+-----+\n",
            "|product_name|year|price|\n",
            "+------------+----+-----+\n",
            "|       Apple|2011| 9000|\n",
            "|       Nokia|2008| 5000|\n",
            "|       Nokia|2009| 5000|\n",
            "+------------+----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SgiGADMrxWoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# #8 [Customer Who Visited but Did Not Make Any Transactions](https://leetcode.com/problems/customer-who-visited-but-did-not-make-any-transactions/?envType=study-plan-v2&envId=top-sql-50)"
      ],
      "metadata": {
        "id": "OBPiTCHVzpry"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pyspark Data Schema"
      ],
      "metadata": {
        "id": "8tPxwRlt0Ebz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.master(\"local[1]\").appName(\"example\").getOrCreate()\n",
        "\n",
        "# Provided data for visits DataFrame\n",
        "visits_data = [\n",
        "    [1, 23],\n",
        "    [2, 9],\n",
        "    [4, 30],\n",
        "    [5, 54],\n",
        "    [6, 96],\n",
        "    [7, 54],\n",
        "    [8, 54]\n",
        "]\n",
        "\n",
        "# Provided data for transactions DataFrame\n",
        "transactions_data = [\n",
        "    [2, 5, 310],\n",
        "    [3, 5, 300],\n",
        "    [9, 5, 200],\n",
        "    [12, 1, 910],\n",
        "    [13, 2, 970]\n",
        "]\n",
        "\n",
        "# Create visits DataFrame\n",
        "visits_df = spark.createDataFrame(visits_data, schema=['visit_id', 'customer_id'])\n",
        "\n",
        "# Create transactions DataFrame\n",
        "transactions_df = spark.createDataFrame(transactions_data, schema=['transaction_id', 'visit_id', 'amount'])\n",
        "\n",
        "# Show the DataFrames\n",
        "visits_df.show()\n",
        "transactions_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPHpssVIzrX5",
        "outputId": "27664040-24d1-4abd-b0ac-80d4477414ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----------+\n",
            "|visit_id|customer_id|\n",
            "+--------+-----------+\n",
            "|       1|         23|\n",
            "|       2|          9|\n",
            "|       4|         30|\n",
            "|       5|         54|\n",
            "|       6|         96|\n",
            "|       7|         54|\n",
            "|       8|         54|\n",
            "+--------+-----------+\n",
            "\n",
            "+--------------+--------+------+\n",
            "|transaction_id|visit_id|amount|\n",
            "+--------------+--------+------+\n",
            "|             2|       5|   310|\n",
            "|             3|       5|   300|\n",
            "|             9|       5|   200|\n",
            "|            12|       1|   910|\n",
            "|            13|       2|   970|\n",
            "+--------------+--------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution"
      ],
      "metadata": {
        "id": "3RKeccw70Cuw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_visits_trans_df=visits_df.join(transactions_df, how='left', on='visit_id').groupby(\n",
        "                                                                  ['customer_id','visit_id']).agg(\n",
        "                                                                  F.count('transaction_id').alias('transactions_count'))\n",
        "user_visits_trans_df.where(F.col('transactions_count')==0).groupby(['customer_id']).agg(F.count('customer_id').alias('count_no_trans')).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRuijhUQzsGH",
        "outputId": "c7738966-a2e6-46fb-f8b2-9850c9366cc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------------+\n",
            "|customer_id|count_no_trans|\n",
            "+-----------+--------------+\n",
            "|         54|             2|\n",
            "|         96|             1|\n",
            "|         30|             1|\n",
            "+-----------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qLuhMjZA5Ulx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}